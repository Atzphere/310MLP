{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6f3be6-263d-401a-8a4d-00822ea38162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import model\n",
    "model = reload(model)\n",
    "\n",
    "# import tf_keras as keras\n",
    "import keras\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import crystal_loader\n",
    "\n",
    "from tqdm import tqdm\n",
    "import tqdm.keras\n",
    "\n",
    "import numpy as np\n",
    "from symmetry import *\n",
    "import dill\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from MLPtools import scale_ragged, atomic_MSE\n",
    "\n",
    "from keras.losses import MeanSquaredError\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "import multiprocess as mp\n",
    "\n",
    "tf.config.run_functions_eagerly(False)\n",
    "\n",
    "try:\n",
    "  physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "  print(\"GPU:\", tf.config.list_physical_devices('GPU'))\n",
    "  print(\"Num GPUs:\", len(physical_devices))\n",
    "except:\n",
    "  print(\"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0f6224-b321-452b-b50a-205a74e29727",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_name = \"TiO2_2015_angfixed_x3\"\n",
    "\n",
    "features_path = f\"../pickles/{dset_name}_features.h5\"\n",
    "labels_path = f\"../pickles/{dset_name}_labeldata.h5\"\n",
    "\n",
    "\n",
    "\n",
    "with h5py.File(features_path, \"r\") as f:\n",
    "    features = [f[f\"array_{i}\"][:] for i in range(len(f))]\n",
    "\n",
    "label_df = pd.read_hdf(labels_path, key=\"labels\")\n",
    "n_atoms = pd.read_hdf(labels_path, key=\"n_atoms\")\n",
    "\n",
    "# (labels.div(n_atoms, axis=\"rows\"))\n",
    "print(label_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b187424-47e3-4a00-b2fc-8890168dafaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select label to use\n",
    "labels = label_df[\"cohesive_energy\"].to_numpy().reshape(-1, 1)\n",
    "\n",
    "scaled_features = scale_ragged(features)\n",
    "\n",
    "Xtrain, Xtest, y_train, y_test, c_train, c_test = train_test_split(scaled_features, labels, n_atoms, shuffle=True, random_state=12, test_size=0.2)\n",
    "Xval, Xtest, y_val, y_test, c_val, c_test = train_test_split(Xtest, y_test, c_test, shuffle=True, random_state=12, test_size=0.5)\n",
    "\n",
    "Xtrain = tf.ragged.constant(Xtrain, ragged_rank=1, inner_shape=(70,))\n",
    "Xval = tf.ragged.constant(Xval, ragged_rank=1, inner_shape=(70,))\n",
    "Xtest = tf.ragged.constant(Xtest, ragged_rank=1, inner_shape=(70,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51feeba5-89b9-4128-bcc6-5908bdca38eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_MLP(n_neurons=10, learning_rate=0.0004, atomic_loss=True, activation=\"relu\"):\n",
    "    layers = [keras.layers.Dense(n_neurons, activation=activation),\n",
    "              keras.layers.Dense(n_neurons, activation=activation)]\n",
    "    if atomic_loss:\n",
    "        ULM = MeanSquaredError()\n",
    "        LM = None\n",
    "    else:\n",
    "        ULM = None\n",
    "        LM = \"mse\"\n",
    "\n",
    "    MLP1 = model.MLPNet(layers=layers,\n",
    "                        N_features=70,\n",
    "                        ragged_processing=False,\n",
    "                        unitwise_loss_model=ULM\n",
    "    )\n",
    "\n",
    "    MLP1.compile(\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=LM\n",
    "    )\n",
    "\n",
    "    return MLP1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c8b4ef-0df0-4edf-aa53-6121fee77995",
   "metadata": {},
   "source": [
    "# Parameter search 1\n",
    "Grid search over neuron count per hidden layer (extreme ends of the aenet paper params), as well as various batch sizes and a different loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cbed76-f5d0-4274-99a7-c48c084377fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP = KerasRegressor(build_MLP, batch_size=32, epochs=200, n_neurons=10, learning_rate=0.0004, atomic_loss=True, activation=\"relu\")\n",
    "\n",
    "params = dict(n_neurons=[10, 50],\n",
    "              learning_rate=[0.0004],\n",
    "              batch_size=[16, 32, 64],\n",
    "              atomic_loss=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5a746a-5b1b-495f-acdb-9cf4aff4f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Get all keys and all value combinations\n",
    "keys = list(params.keys())\n",
    "values = list(params.values())\n",
    "\n",
    "# Create list of dictionaries for each combination\n",
    "search_grid = [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "# Optionally print or inspect\n",
    "for config in search_grid:\n",
    "    print(config)\n",
    "\n",
    "def without_keys(d, keys):\n",
    "    return {x: d[x] for x in d if x not in keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04721fa0-aa29-4c36-826c-989efc507ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for config in search_grid:\n",
    "    parameters = without_keys(config, \"batch_size\")\n",
    "    MLP = build_MLP(**parameters)\n",
    "    #seed = np.random.randint(1, 20000000)\n",
    "    res = MLP.fit(\n",
    "        # tf.random.shuffle(Xtrain, seed=seed), tf.random.shuffle(y_train, seed=seed),\n",
    "        Xtrain, y_train,\n",
    "        batch_size = config[\"batch_size\"],\n",
    "        epochs = 250,\n",
    "        verbose = 0\n",
    "    )\n",
    "\n",
    "    train_score = MLP.evaluate(Xtrain, y_train)\n",
    "    test_score = MLP.evaluate(Xval, y_val)\n",
    "\n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee47b6-46c7-4221-9124-84305be88d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for config, train_score, test_score in zip(search_grid, train_scores, test_scores):\n",
    "    print(config)\n",
    "    print(f\"train score: {train_score:.4f}, test score: {test_score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6accd8a-d126-4d9a-bc54-ec70fdce3fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d50a2f3-1cb3-4939-abda-e436a7597e55",
   "metadata": {},
   "source": [
    "Atomic loss doesn't seem to be permitting any learning to occur; losses in every trial bottomed out at 30 eV **per atom** which is wildly higher than the non-atomic loss MSE (although one should note that the two losses are different and not directly comparable). From a single fold, the best model appears to be one runnning with 70-50-50-1 subnet architecture and a batch size of 32. I would run this with more folds to validate these results, but this puts my computer out of commission for a few days and I need to study..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8e1297-dad4-4291-b4cf-59814479e40b",
   "metadata": {},
   "source": [
    "## Round 2 of Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95323820-dfe8-4237-b660-56436c2f7cf2",
   "metadata": {},
   "source": [
    "Testing different learning rates, as well as neuron counts again. I have reduced the total number of epochs as I am more interested in how quickly things converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade45691-af00-4d64-ad54-baca5116ecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_2 = dict(n_neurons=[10, 50],\n",
    "              learning_rate=[0.0004, 0.004, 0.00004],\n",
    "              batch_size=[32],\n",
    "              atomic_loss=[False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a4963e-ae73-409d-b3ff-7f4fcc7310f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all keys and all value combinations\n",
    "keys = list(params_2.keys())\n",
    "values = list(params_2.values())\n",
    "\n",
    "# Create list of dictionaries for each combination\n",
    "search_grid_2 = [dict(zip(keys, v)) for v in product(*values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06567b94-4e21-497a-b733-80bcbe9897bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_r2 = []\n",
    "train_scores_r2 = []\n",
    "test_scores_r2 = []\n",
    "\n",
    "for config in search_grid_2:\n",
    "    parameters = without_keys(config, \"batch_size\")\n",
    "    MLP = build_MLP(**parameters)\n",
    "    #seed = np.random.randint(1, 20000000)\n",
    "    res = MLP.fit(\n",
    "        Xtrain, y_train,\n",
    "        batch_size = config[\"batch_size\"],\n",
    "        epochs = 100,\n",
    "        verbose = 0\n",
    "    )\n",
    "\n",
    "    train_score = MLP.evaluate(Xtrain, y_train)\n",
    "    test_score = MLP.evaluate(Xval, y_val)\n",
    "\n",
    "    train_scores_r2.append(train_score)\n",
    "    test_scores_r2.append(test_score)\n",
    "    results_r2.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a6092a-2c1a-4367-8378-2baf7d0c8faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for config, train_score, test_score in zip(search_grid_2, train_scores_r2, test_scores_r2):\n",
    "    print(config)\n",
    "    print(f\"train score: {train_score:.4f}, test score: {test_score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23131115-e5f4-49ae-ae04-0dbeaf5dc536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(num=1, figsize=(10, 8))\n",
    "for num, (result, sets) in enumerate(zip(results_r2, search_grid_2)):\n",
    "    ax.plot(result.epoch, result.history[\"loss\"], label=str(sets))\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylabel(\"MSE loss $(eV^2)$\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xlabel(\"Epoch\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
